## Essential questions

- Once a ML model has been developed, it must be [**deployed**]{.orange} to [**serve**]{.orange} its end users
    - Which [**production**]{.blue2} infrastructure ?
    - Who are the [**end users**]{.blue2} ?
    - [**Batch**]{.blue2} serving vs. [**online**]{.blue2} serving

## Envisioned configuration

- The model might serve [**various applications**]{.orange}
    - Make the model accessible via an [**API**]{.blue2}

- [**Online serving**]{.orange}
    - [**Client**]{.blue2} applications send a [**request**]{.blue2} to the API and get a fast [**response**]{.blue2}

- Production infrastructure : [**Kubernetes**]{.orange} cluster

## Exposing a model through an API

![](../img/API.png){fig-align="center"}

## Why expose a model via a REST API?

- [**Simplicity**]{.orange}: single entry point that hides the underlying complexity of the model

- [**Standardization**]{.orange}: HTTP requests -> agnostic to the programming language used

- [**Scalability**]{.orange}: adapts to the load of concurrent requests

- [**Modularity**]{.orange}: separation of model management and its availability

## Exposing a model through an API

![](../img/API.png){fig-align="center"}

## Run the API in a container

- [**Container**]{.orange}: [**self-contained**]{.blue2} and [**isolated**]{.blue2} environment that encapsulates the model, its dependencies and the API code

- [**Advantages**]{.orange}:  
  - [**Portability**]{.blue2}
  - [**Scalability**]{.blue2} to efficiently distribute the model

- [**Technical prerequisites**]{.orange} for deploying on `Kubernetes`

## Deploying an API on `Kubernetes`

![](../img/ci-cd.png){fig-align="center"}
