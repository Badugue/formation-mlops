## Model serving

- Once a ML model has been developed, it must be [**deployed**]{.orange} to [**serve**]{.orange} its end users
    - Which [**production**]{.blue2} infrastructure ?
    - Who are the [**end users**]{.blue2} ?
    - [**Batch**]{.blue2} serving vs. [**online**]{.blue2} serving

## A standard setup

- Production infrastructure : [**Kubernetes**]{.orange} cluster

- The model might serve [**various applications**]{.orange}
    - Make the model accessible via an [**API**]{.blue2}

- [**Online serving**]{.orange}
    - [**Client**]{.blue2} applications send a [**request**]{.blue2} to the API and get a fast [**response**]{.blue2}

## Exposing a model through an API

![](img/API.png){fig-align="center"}

## Run the API in a container

- [**Container**]{.orange}: [**self-contained**]{.blue2} and [**isolated**]{.blue2} environment that encapsulates the model, its dependencies and the API code

- Containers provide high [**portability**]{.orange} and [**scalability**]{.orange} for distributing the model efficiently.

- The `Dockerfile` is used to configure and build the Docker container.

## Development with Docker architecture

![](img/docker-workflow.png){fig-align="center" width="87%"}

::: aside
Source: [R. Krispin](https://github.com/RamiKrispin/vscode-python)
:::


## Deploying an API on `Kubernetes`

<!-- - Need a [**container orchestration tool**]{.orange} such as kubernetes -->
- [**3 main files**]{.orange} are needed to deploy an API:
    - `deployment.yaml` : defines how the API should [**run**]{.blue2} (container image, resources, and environment variables)
    - `service.yaml` : establishes a stable [**internal**]{.blue2} network endpoint for the API.
    - `ingress.yaml` : provides an entry point for [**external**]{.blue2} clients to access the API.
